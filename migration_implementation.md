# Anki AI Chat Tool - LiteLLM è¿ç§»å®ç°æ–‡æ¡£

## å®ç°ç­–ç•¥
é‡‡ç”¨æ¸è¿›å¼è¿ç§»ç­–ç•¥ï¼Œæœ€å°åŒ–ä»£ç ä¿®æ”¹ï¼Œä¿æŒå‘åå…¼å®¹æ€§ã€‚

## å®ç°æ­¥éª¤

### æ­¥éª¤ 1ï¼šä¾èµ–ç®¡ç†å’Œç¯å¢ƒå‡†å¤‡

#### 1.1 å®‰è£… LiteLLM
```bash
pip install litellm
```

#### 1.2 åˆ›å»ºä¾èµ–æ–‡ä»¶
```python
# requirements.txt (æ–°å»º)
litellm>=1.0.0
openai>=1.0.0  # ä¿æŒå‘åå…¼å®¹
requests>=2.25.0
```

### æ­¥éª¤ 2ï¼šåˆ›å»ºç»Ÿä¸€ AI æœåŠ¡

#### 2.1 æ–°å»º `services/unified_ai_service.py`
```python
# ä¼ªä»£ç 
import litellm
from typing import List, Dict, Any, Optional

class UnifiedAIService:
    def __init__(self):
        self.config = Config.get_ai_config()
        self.setup_litellm()
        self.setup_providers()
    
    def setup_litellm(self):
        # é…ç½® LiteLLM å…¨å±€è®¾ç½®
        litellm.set_verbose = self.config.get("debug_mode", False)
        litellm.drop_params = True  # è‡ªåŠ¨å¤„ç†ä¸æ”¯æŒçš„å‚æ•°
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        litellm.num_retries = self.config.get("retry_attempts", 3)
        litellm.request_timeout = self.config.get("timeout", 30)
    
    def setup_providers(self):
        # è®¾ç½®ç¯å¢ƒå˜é‡
        import os
        if self.config.get("openai_api_key"):
            os.environ["OPENAI_API_KEY"] = self.config["openai_api_key"]
        if self.config.get("anthropic_api_key"):
            os.environ["ANTHROPIC_API_KEY"] = self.config["anthropic_api_key"]
    
    def get_response(self, conversation_history: List[Dict]) -> str:
        """è·å– AI å›å¤ - ä¿æŒåŸæœ‰æ¥å£"""
        if not conversation_history:
            return self._handle_error("Empty conversation history")
        
        try:
            # è·å–å½“å‰æ¨¡å‹é…ç½®
            model = self._get_current_model()
            
            # è°ƒç”¨ LiteLLM
            response = litellm.completion(
                model=model,
                messages=conversation_history,
                max_tokens=self.config.get("max_tokens", 500),
                temperature=self.config.get("temperature", 0.7)
            )
            
            # æå–å›å¤å†…å®¹
            return self._extract_content(response)
            
        except Exception as e:
            return self._handle_error(str(e))
    
    def _get_current_model(self) -> str:
        """è·å–å½“å‰æ¨¡å‹"""
        provider = self.config.get("ai_provider", "openai")
        model = self.config.get("openai_model", "gpt-3.5-turbo")
        
        if provider == "openai":
            return model
        elif provider == "anthropic":
            return "claude-3-sonnet-20240229"
        elif provider == "google":
            return "gemini-pro"
        else:
            return model  # é»˜è®¤è¿”å› OpenAI æ¨¡å‹
    
    def _extract_content(self, response) -> str:
        """æå–å“åº”å†…å®¹"""
        if hasattr(response, 'choices') and response.choices:
            choice = response.choices[0]
            if hasattr(choice, 'message') and choice.message.content:
                return choice.message.content.strip()
        
        return self._handle_error("Empty response from API")
    
    def _handle_error(self, error_msg: str) -> str:
        """é”™è¯¯å¤„ç† - ä¿æŒåŸæœ‰æ ¼å¼"""
        return f"AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {error_msg}"
    
    def validate_api_key(self) -> tuple[bool, str]:
        """éªŒè¯ API å¯†é’¥"""
        try:
            test_messages = [{"role": "user", "content": "Hello"}]
            response = litellm.completion(
                model=self._get_current_model(),
                messages=test_messages,
                max_tokens=10
            )
            return True, "API key is valid"
        except Exception as e:
            return False, f"API key validation failed: {str(e)}"
    
    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "ai_provider": self.config.get("ai_provider", "openai"),
            "model": self._get_current_model(),
            "api_key_set": bool(self._get_api_key()),
            "max_tokens": self.config.get("max_tokens", 500),
            "temperature": self.config.get("temperature", 0.7),
            "retry_attempts": self.config.get("retry_attempts", 3)
        }
    
    def _get_api_key(self) -> Optional[str]:
        """è·å–å½“å‰æä¾›å•†çš„ API å¯†é’¥"""
        provider = self.config.get("ai_provider", "openai")
        if provider == "openai":
            return self.config.get("openai_api_key")
        elif provider == "anthropic":
            return self.config.get("anthropic_api_key")
        return None
```

### æ­¥éª¤ 3ï¼šæ‰©å±•é…ç½®ç®¡ç†

#### 3.1 æ›´æ–° `config.py`
```python
# ä¼ªä»£ç  - åœ¨ç°æœ‰ Config ç±»ä¸­æ·»åŠ 
class Config:
    DEFAULT_CONFIG = {
        # ä¿æŒç°æœ‰é…ç½®
        "openai_api_key": "sk-placeholder-key-here",
        "openai_model": "gpt-3.5-turbo",
        "max_tokens": 500,
        "temperature": 0.7,
        
        # æ–°å¢ç»Ÿä¸€ AI é…ç½®
        "ai_provider": "openai",  # ä¸»è¦æä¾›å•†
        "fallback_providers": [],  # å›é€€æä¾›å•†åˆ—è¡¨
        
        # å¤šæä¾›å•† API å¯†é’¥
        "anthropic_api_key": "",
        "google_api_key": "",
        
        # å¢å¼ºé…ç½®
        "retry_attempts": 3,
        "timeout": 30,
        "enable_cost_tracking": False,
        
        # ä¿æŒç°æœ‰é…ç½®
        "chat_window_width": 600,
        "chat_window_height": 400,
        "save_conversations": True,
        "conversation_separator": "<hr><h3>AI Chat History</h3>",
        "debug_mode": False
    }
    
    @classmethod
    def get_ai_config(cls):
        """è·å– AI ç›¸å…³é…ç½®"""
        config = cls.get_config()
        return {
            "ai_provider": config.get("ai_provider", "openai"),
            "openai_api_key": config.get("openai_api_key", "sk-placeholder-key-here"),
            "openai_model": config.get("openai_model", "gpt-3.5-turbo"),
            "anthropic_api_key": config.get("anthropic_api_key", ""),
            "google_api_key": config.get("google_api_key", ""),
            "max_tokens": config.get("max_tokens", 500),
            "temperature": config.get("temperature", 0.7),
            "retry_attempts": config.get("retry_attempts", 3),
            "timeout": config.get("timeout", 30),
            "debug_mode": config.get("debug_mode", False)
        }
    
    @classmethod
    def get_openai_config(cls):
        """ä¿æŒå‘åå…¼å®¹çš„ OpenAI é…ç½®æ–¹æ³•"""
        ai_config = cls.get_ai_config()
        return {
            "api_key": ai_config["openai_api_key"],
            "model": ai_config["openai_model"],
            "max_tokens": ai_config["max_tokens"],
            "temperature": ai_config["temperature"]
        }
```

### æ­¥éª¤ 4ï¼šåˆ›å»ºé€‚é…å™¨æ¨¡å¼

#### 4.1 æ›´æ–° `services/openai_service.py`
```python
# ä¼ªä»£ç  - ä¿æŒå‘åå…¼å®¹
from .unified_ai_service import UnifiedAIService

class OpenAIService:
    """OpenAI æœåŠ¡é€‚é…å™¨ - ä¿æŒå‘åå…¼å®¹"""
    
    def __init__(self):
        # ä½¿ç”¨ç»Ÿä¸€ AI æœåŠ¡ä½œä¸ºåç«¯
        self.unified_service = UnifiedAIService()
        
        # ä¿æŒåŸæœ‰å±æ€§ç”¨äºå…¼å®¹æ€§
        config = Config.get_openai_config()
        self.api_key = config.get("api_key")
        self.model = config.get("model")
        self.max_tokens = config.get("max_tokens")
        self.temperature = config.get("temperature")
    
    def get_response(self, conversation_history):
        """å§”æ‰˜ç»™ç»Ÿä¸€æœåŠ¡"""
        return self.unified_service.get_response(conversation_history)
    
    def validate_api_key(self):
        """å§”æ‰˜ç»™ç»Ÿä¸€æœåŠ¡"""
        return self.unified_service.validate_api_key()
    
    def get_service_status(self):
        """é€‚é…ç»Ÿä¸€æœåŠ¡çŠ¶æ€åˆ°åŸæœ‰æ ¼å¼"""
        status = self.unified_service.get_service_status()
        return {
            "openai_available": True,  # LiteLLM å¤„ç†å¯ç”¨æ€§
            "api_key_set": status["api_key_set"],
            "model": status["model"],
            "max_tokens": status["max_tokens"],
            "temperature": status["temperature"]
        }
    
    def update_config(self, new_config):
        """æ›´æ–°é…ç½® - ä¿æŒå…¼å®¹æ€§"""
        if "api_key" in new_config:
            self.api_key = new_config["api_key"]
        if "model" in new_config:
            self.model = new_config["model"]
        if "max_tokens" in new_config:
            self.max_tokens = new_config["max_tokens"]
        if "temperature" in new_config:
            self.temperature = new_config["temperature"]
        
        # åŒæ­¥åˆ°ç»Ÿä¸€é…ç½®
        # è¿™é‡Œéœ€è¦å®ç°é…ç½®åŒæ­¥é€»è¾‘
```

### æ­¥éª¤ 5ï¼šæµ‹è¯•è„šæœ¬åˆ›å»º

#### 5.1 åˆ›å»º `test_unified_ai_service.py`
```python
# ä¼ªä»£ç 
#!/usr/bin/env python3
"""
ç»Ÿä¸€ AI æœåŠ¡æµ‹è¯•è„šæœ¬
"""

def test_litellm_installation():
    """æµ‹è¯• LiteLLM å®‰è£…"""
    try:
        import litellm
        print("âœ… LiteLLM å®‰è£…æˆåŠŸ")
        print(f"   ç‰ˆæœ¬: {litellm.__version__}")
        return True
    except ImportError as e:
        print(f"âŒ LiteLLM å®‰è£…å¤±è´¥: {e}")
        return False

def test_unified_service():
    """æµ‹è¯•ç»Ÿä¸€ AI æœåŠ¡"""
    try:
        from services.unified_ai_service import UnifiedAIService
        service = UnifiedAIService()
        print("âœ… ç»Ÿä¸€ AI æœåŠ¡åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•é…ç½®
        status = service.get_service_status()
        print(f"   æä¾›å•†: {status['ai_provider']}")
        print(f"   æ¨¡å‹: {status['model']}")
        
        return True
    except Exception as e:
        print(f"âŒ ç»Ÿä¸€ AI æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    try:
        from services.openai_service import OpenAIService
        service = OpenAIService()
        print("âœ… OpenAI æœåŠ¡å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    print("=" * 60)
    print("ç»Ÿä¸€ AI æœåŠ¡è¿ç§»æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_litellm_installation,
        test_unified_service,
        test_backward_compatibility
    ]
    
    results = []
    for test in tests:
        print(f"\n{test.__doc__}...")
        results.append(test())
    
    print("\n" + "=" * 60)
    if all(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¿ç§»å‡†å¤‡å°±ç»ª")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    print("=" * 60)

if __name__ == "__main__":
    main()
```

## å®æ–½æ—¶é—´çº¿

### ç¬¬ 1 å¤©ï¼šç¯å¢ƒå‡†å¤‡
- å®‰è£… LiteLLM
- åˆ›å»ºåŸºç¡€æ–‡ä»¶ç»“æ„
- è¿è¡Œåˆå§‹æµ‹è¯•

### ç¬¬ 2 å¤©ï¼šæ ¸å¿ƒå®ç°
- å®ç° UnifiedAIService
- æ›´æ–°é…ç½®ç®¡ç†
- åˆ›å»ºé€‚é…å™¨

### ç¬¬ 3 å¤©ï¼šæµ‹è¯•å’ŒéªŒè¯
- åˆ›å»ºæµ‹è¯•è„šæœ¬
- éªŒè¯åŠŸèƒ½å®Œæ•´æ€§
- ä¿®å¤å‘ç°çš„é—®é¢˜

### ç¬¬ 4 å¤©ï¼šä¼˜åŒ–å’Œæ–‡æ¡£
- æ€§èƒ½ä¼˜åŒ–
- æ›´æ–°æ–‡æ¡£
- æœ€ç»ˆæµ‹è¯•

## å›æ»šè®¡åˆ’
å¦‚æœè¿ç§»å‡ºç°é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å›æ»šï¼š
1. ä¿æŒåŸæœ‰ `openai_service.py` ä¸å˜
2. åœ¨é…ç½®ä¸­æ·»åŠ  `use_legacy_service: true` é€‰é¡¹
3. æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨æ–°æ—§æœåŠ¡
